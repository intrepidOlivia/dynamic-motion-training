# dynamic-motion-training
Project for SeattleVR's Healthcare Hackathon 2018

## Inspiration
In Sean's experience with physical therapy and Andy's experience with dance, both project designers have recognized a need for motion instruction that is more intuitive and thorough than static words on a page. Virtual reality is a medium with as-yet unrealized potential for virtualizing human movement, and so this project is an experiment in tracking and instructing correct motion.

## What it does
For the first segment, the exercise is demonstrated to the user via an animated 3D character, which can visually display the movement intuitively in physical space. When the user is ready to continue, they are embodied in a 3D avatar where they can perform the exercise themselves. Visual indicators light up green when the movement is performed correctly, providing feedback to the user about the accuracy of their motion.

## How we built it
We developed our project in Unity using the SteamVR plugin and using an HTC Vive with additional Vive trackers.

## Challenges we ran into
Design challenges included configuring a scene with multiple trackers to enable tracking of multiple body parts, as well as representing user movement with a humanoid avatar.

##Resources used:
[Vive IK Demo](https://github.com/JamesBear/vive_ik_demo)

[Robot Kyle](https://assetstore.unity.com/packages/3d/characters/robots/space-robot-kyle-4696)

[Steam VR Asset Pack](https://assetstore.unity.com/packages/templates/systems/steamvr-plugin-32647)

[SteamVR Laser Pointer Script](https://unity3d.college/2017/06/17/steamvr-laser-pointer-menus/)

[Hand Painted Nature Kit](https://assetstore.unity.com/packages/3d/environments/hand-painted-nature-kit-lite-69220)
